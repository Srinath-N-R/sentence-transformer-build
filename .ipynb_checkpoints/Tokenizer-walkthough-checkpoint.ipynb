{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df0e215-b8c5-46ce-bd06-62599e5f8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3b539a-d698-4c3a-a729-fd882c19ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Loading Wikipedia dataset, just 1% of the data\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:1%]\")\n",
    "\n",
    "# Saving as text file\n",
    "with open(\"wikipedia_train.txt\", \"w\", encoding='utf-8') as f:\n",
    "    for item in dataset:\n",
    "        f.write(item[\"text\"].replace(\"\\n\", \" \") + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e9b2b-27be-4ed3-9e8f-c2a77b982298",
   "metadata": {},
   "source": [
    "## Motivation for BPE Tokenization\n",
    "We are implementing a Byte Pair Encoding Tokenizer. There are a range of tokenizing options like Word Piece tokenizer or a simple character lookup tokenizer. These tokenization techniques are simpler to implement, however they are ineffecient because they use more tokens to represent sentences when compared to BPE tokenizer.\n",
    "\n",
    "BPE tokenizer is a state of the art technique to represent sentences and it is used in models like GPT-2. The high level overview of BPE is that, frequently occuring tokens will be merged to form a singular new token rather represnting the component parts as multiple individual tokens. To learn more about BPE, check this out: https://en.wikipedia.org/wiki/Byte_pair_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55459ce-9f21-4c94-a8ee-da52441cee53",
   "metadata": {},
   "source": [
    "### This is notebook primarily focussed on training the Tokenizer and to show the process more intuitively. Once trained, this tolenizer can be called from a Tokenizer class which is defined in the tokenizer.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a791313-e872-4013-8204-3aa94d23476a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|███████████████████████████▉| 897M/900M [03:00<00:00, 4.96MB/s]\n",
      "Counting Pairs: 100%|██████| 315181836/315181836 [04:57<00:00, 1060373.97seqs/s]\n",
      "Merging: 100%|██████████████████████████████| 743/743 [2:02:49<00:00,  9.92s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Config\n",
    "file_path = 'wikipedia_train.txt'\n",
    "chunk_size = 10000  # read the file in chunks of 0.01 MB to keep the calculations in memory.\n",
    "vocab_size = 997 # ideally the vocab size would be 1000 after we add CLS and PAD tokens.\n",
    "orig_vocab_size = 255 # since we are using utf-8, values 0-255 are blocked.\n",
    "new_token = orig_vocab_size  # values from 256-997 are free to be assigned.\n",
    "merged = {} # holds the combination of tokens that merge to form a new token.\n",
    "\n",
    "# Tokenizer Pattern\n",
    "# This pattern is a slight modification of the GPT-2 pattern to split sentences based on space characters. \n",
    "# This modification is necessary for downstream tasks like NER where spaces between words are better left uncombined.\n",
    "pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d|\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+|\\s+\"\"\")\n",
    "\n",
    "\n",
    "# Read text file in chunks. And tokenize the text chunks. Then update the tokens in a global list, token_seqs. \n",
    "# token_seqs is a list of list of tokens. This is because, when the text goes through the above regex pattern, it is split into lists.\n",
    "# Then Tokenization & token merging will happen inside these sublists. \n",
    "# This sentence representation is more suitable for downstream tasks like NER.\n",
    "\n",
    "file_size = os.path.getsize(file_path)\n",
    "token_seqs = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f, tqdm(total=file_size, unit='B', unit_scale=True, desc=\"Tokenizing\") as pbar:\n",
    "    while True:\n",
    "        chunk = f.read(chunk_size)\n",
    "        if not chunk:\n",
    "            break\n",
    "        texts = re.findall(pat, chunk)\n",
    "        token_seqs.extend([list(token.encode('utf-8')) for token in texts])\n",
    "        pbar.update(len(chunk))\n",
    "\n",
    "\n",
    "def build_pair_stats(seqs):\n",
    "    '''\n",
    "    Stats to calculate the count of token-pairs and their indices in token_seqs. \n",
    "    Knowing token-pairs' indices will help in faster updates during merging.\n",
    "    '''\n",
    "    \n",
    "    pair_counts = Counter()\n",
    "    pair_to_indices = defaultdict(set)\n",
    "\n",
    "    with tqdm(total=len(seqs), desc=\"Counting Pairs\", unit=\"seqs\") as pbar:\n",
    "        for idx, seq in enumerate(seqs):\n",
    "            for i in range(len(seq) - 1):\n",
    "                pair = (seq[i], seq[i + 1])\n",
    "                pair_counts[pair] += 1\n",
    "                pair_to_indices[pair].add(idx)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return pair_counts, pair_to_indices\n",
    "\n",
    "\n",
    "def merge_sequence(seq, tok_1, tok_2, new_token):\n",
    "    '''\n",
    "    Function to merge tokens.\n",
    "    For a given sequence, look for the token pair (tok_1, tok_2) that needs to be updated.\n",
    "    Then update the token pair with the new token (new_token).\n",
    "    Return the merged sequence.\n",
    "    '''\n",
    "    \n",
    "    merged_seq = []\n",
    "    i = 0\n",
    "    while i < len(seq):\n",
    "        if i < len(seq) - 1 and seq[i] == tok_1 and seq[i + 1] == tok_2:\n",
    "            merged_seq.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_seq.append(seq[i])\n",
    "            i += 1\n",
    "    return merged_seq\n",
    "\n",
    "# Initial Pair Stats\n",
    "pair_counts, pair_to_indices = build_pair_stats(token_seqs)\n",
    "\n",
    "# BPE Merge Loop\n",
    "# The merge loop will run n number of times, where n is the number of new tokens to be created.\n",
    "# Since we need vocab of 997 and we already have 0-225 vocab values assigned, run this loop 997-255 times to generate 998 len vocab.\n",
    "for _ in tqdm(range(vocab_size - orig_vocab_size), desc=\"Merging\"):\n",
    "    if not pair_counts:\n",
    "        break\n",
    "\n",
    "    # Find most frequent unmerged pair\n",
    "    for (tok_1, tok_2), _ in pair_counts.most_common():\n",
    "        if (tok_1, tok_2) not in merged:\n",
    "            break\n",
    "\n",
    "    # Create new token value and store it in the merged map\n",
    "    new_token += 1\n",
    "    merged[(tok_1, tok_2)] = new_token\n",
    "\n",
    "    # For the token to be merged, get all affected indices\n",
    "    affected_indices = pair_to_indices.get((tok_1, tok_2), set())\n",
    "    if not affected_indices:\n",
    "        continue\n",
    "\n",
    "    # Merge only affected sequences\n",
    "    for idx in list(affected_indices):\n",
    "        old_seq = token_seqs[idx]\n",
    "        new_seq = merge_sequence(old_seq, tok_1, tok_2, new_token)\n",
    "        token_seqs[idx] = new_seq\n",
    "    \n",
    "        # Because we merged, decrement the pair counts from \"old sequence\" and delete their respective indices that were updated.\n",
    "        for i in range(len(old_seq) - 1):\n",
    "            pair = (old_seq[i], old_seq[i + 1])\n",
    "            pair_counts[pair] -= 1\n",
    "            if pair_counts[pair] <= 0:\n",
    "                del pair_counts[pair]\n",
    "            pair_to_indices[pair].discard(idx)\n",
    "            if not pair_to_indices[pair]:\n",
    "                del pair_to_indices[pair]\n",
    "\n",
    "        # Because we merged, increment the pair counts from \"new sequence\" and add their respective indices that were updated.\n",
    "        for i in range(len(new_seq) - 1):\n",
    "            pair = (new_seq[i], new_seq[i + 1])\n",
    "            pair_counts[pair] += 1\n",
    "            pair_to_indices[pair].add(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e750fbf-d138-4f6a-8371-601f2fa9e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reverse map of the merged items.\n",
    "rev_merged = {v:k for k, v in merged.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04acb58b-0c2b-457a-af96-8fbf73b709b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(list_of_tokens):\n",
    "    '''\n",
    "    Count the frequency of adjacent token pairs across a list of token sequences.\n",
    "\n",
    "    Notes:\n",
    "        This function is a simplified version of build_pair_stats, intended for sentence-level inference,\n",
    "        where maximum efficiency is not critical.\n",
    "    '''\n",
    "    counts = {}\n",
    "\n",
    "    for token_list in list_of_tokens:\n",
    "        for pair in zip(token_list, token_list[1:]):\n",
    "            if pair not in counts:\n",
    "                counts[pair] = 1\n",
    "            else:\n",
    "                counts[pair] += 1\n",
    "    \n",
    "    count_vals = [(v, k[0], k[1]) for k,v in counts.items()]\n",
    "    count_vals = sorted(count_vals, reverse=True)\n",
    "    return count_vals\n",
    "\n",
    "    \n",
    "def encode(text):\n",
    "    '''\n",
    "    Encode a text sequence by iteratively merging token pairs based on a predefined merge map.\n",
    "\n",
    "    Process:\n",
    "        - Split the input text into fragments.\n",
    "        - Tokenize each fragment into byte-encoded tokens.\n",
    "        - Iteratively merge token pairs found in the 'merged' mapping, \n",
    "          prioritizing pairs with the smallest assigned merged token value.\n",
    "        - Continue merging until no more applicable pairs are found.\n",
    "\n",
    "    Notes:\n",
    "        Priority is given to token pairs with the lowest merged value to preserve merge coherence.\n",
    "    '''\n",
    "    texts = re.findall(pat, text)\n",
    "    list_of_tokens = [list(text.encode('utf-8')) for text in texts]\n",
    "\n",
    "    seen = set()\n",
    "    while True:\n",
    "        count_vals = get_counts(list_of_tokens)\n",
    "        pairs = [(val[1:]) for val in count_vals]\n",
    "        pairs = [pair for pair in pairs if pair in merged and pair not in seen]\n",
    "        valid_pairs = {pair:merged[pair] for pair in pairs}\n",
    "        \n",
    "        if valid_pairs:\n",
    "            min_pair = min(valid_pairs, key=lambda x:merged[x])\n",
    "            seen.add(min_pair)\n",
    "            tok_1, tok_2 = min_pair\n",
    "            new_token = merged[(tok_1, tok_2)]\n",
    "\n",
    "            new_list_of_tokens = []\n",
    "    \n",
    "            for tokens in list_of_tokens:\n",
    "                new_tokens = merge_sequence(tokens, tok_1, tok_2, new_token)\n",
    "                new_list_of_tokens.append(new_tokens)\n",
    "            list_of_tokens = new_list_of_tokens\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return list_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bcdf3a6-a815-43bd-8fb1-0958aa052191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_str(number):\n",
    "    '''\n",
    "    Convert an integer (0–255) to its corresponding single UTF-8 character.\n",
    "    '''\n",
    "    if number > 255:\n",
    "        raise ValueError(\"Invalid Token\")\n",
    "    byte_representation = number.to_bytes(1, byteorder='big')\n",
    "    utf8_string = byte_representation.decode('utf-8')\n",
    "    return utf8_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0cf1928-8660-4ec1-9f35-aa64a064320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(seq, tok_1, tok_2, merged_token):\n",
    "    '''\n",
    "    Split merged tokens in a sequence back into their original token pairs using the 'rev_merged' mapping.\n",
    "    '''\n",
    "    new_tokens = []\n",
    "\n",
    "    for token in seq:\n",
    "        if token == merged_token:\n",
    "            tok_1, tok_2 = rev_merged[token]\n",
    "            new_tokens.append(tok_1)\n",
    "            new_tokens.append(tok_2)\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77519889-980f-4b6a-ba82-21ac83d76428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(list_of_tokens):\n",
    "    '''\n",
    "    Decode a list of merged token sequences back into the original text string.\n",
    "    \n",
    "    Process:\n",
    "        - Iteratively split merged tokens into their original token pairs, \n",
    "          starting from the highest merged token value.\n",
    "        - Flatten the final list of tokens.\n",
    "        - Convert each token back to its corresponding UTF-8 character.\n",
    "        - Join the characters into a single decoded string.\n",
    "    \n",
    "    Notes:\n",
    "        Priority is given to token pairs with the highest merged value to preserve un-merge coherence.\n",
    "    '''\n",
    "\n",
    "    while True:\n",
    "        valid_tokens = []\n",
    "        for tokens in list_of_tokens:\n",
    "            valid_tokens.extend([token for token in tokens if token in rev_merged.keys()])\n",
    "\n",
    "        if valid_tokens:\n",
    "            max_token = max(valid_tokens)                 \n",
    "            new_list_of_tokens = []\n",
    "            for tokens in list_of_tokens:\n",
    "                new_tokens = split_sequence(tokens, tok_1, tok_2, max_token)\n",
    "                new_list_of_tokens.append(new_tokens)\n",
    "            list_of_tokens = new_list_of_tokens\n",
    "        else:\n",
    "            break\n",
    "    final_tokens = []\n",
    "    for tokens in list_of_tokens:\n",
    "        final_tokens.extend(tokens)\n",
    "    str_tokens = [num_to_str(token) for token in final_tokens]\n",
    "    return \"\".join(str_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7585ed1-0ec6-4e59-8344-4ebf1cec5ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we assert that an arbitary text, once passed through the encode and decode functions, returns the same input text.\n",
    "inp = '''Through the 1980s, Tarantino had a number of jobs. After lying about his age, he worked as an usher at an adult movie theater in Torrance, called the Pussycat Theater. '''\n",
    "out = decode(encode(inp))\n",
    "inp==out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b870ed9f-4887-4f79-b0a6-b27b85f5d72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized text example  :  [465, 114, 445, 32, 260, 32, 630, 48, 115, 44, 32, 84, 271, 379, 257, 111, 32, 430, 32, 97, 32, 952, 32, 270, 32, 106, 553, 115, 46, 32, 65, 102, 298, 32, 301, 278, 32, 805, 32, 368, 32, 381, 44, 32, 284, 32, 647, 267, 32, 273, 32, 259, 32, 306, 567, 32, 263, 32, 259, 32, 292, 508, 32, 355, 948, 32, 260, 491, 32, 257, 32, 84, 265, 500, 321, 44, 32, 881, 32, 260, 32, 80, 306, 529, 99, 263, 32, 299, 491, 46, 32]\n",
      "Length of original text :  168\n",
      "Length of tokenized text:  91\n",
      "Tokenized efficieny     :  1.85\n"
     ]
    }
   ],
   "source": [
    "encoded = encode(inp)\n",
    "flattened = []\n",
    "for t in encoded: flattened.extend(t)\n",
    "\n",
    "print(\"tokenized text example  : \", flattened)\n",
    "print(\"Length of original text : \", len(inp))\n",
    "print(\"Length of tokenized text: \", len(flattened))\n",
    "print(\"Tokenized efficieny     : \", round(len(inp)/ len(flattened), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aa82567-a070-434c-973b-31f3d9ea8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are saving the merged-map as a json for future reference!\n",
    "import json\n",
    "merged_serializable = {f\"({k[0]},{k[1]})\": v for k, v in merged.items()}\n",
    "with open(\"bpe_merged.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(merged_serializable, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616031e7-37ed-49c8-8877-5960a5bfe8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
