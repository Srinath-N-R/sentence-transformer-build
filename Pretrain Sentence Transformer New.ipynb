{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d2bd3-a0ce-47ea-a384-13a55882a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tokenizer import Tokenizer\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from enums import PAD_ID, MAX_LEN, CLS_ID\n",
    "from sent_transformer import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9a9c7-413b-438b-b17a-a15ec62e9ea3",
   "metadata": {},
   "source": [
    "# Senetence Transformer Overview\n",
    "## Datasets:\n",
    "* STS-B (Semantic Textual Similarity Benchmark)\n",
    "  \n",
    "> Sentences are paired with similarity scores. We use high-score pairs as positives (score ≥ 0.6) and low-score pairs as negatives (score ≤ 0.5).\n",
    "* QQP (Quora Question Pairs)\n",
    "\n",
    "> Provided triplet dataset with queries, positive matches, and negative examples.\n",
    "    \n",
    "\n",
    "#### The two datasets are merged to create a diverse training pool.\n",
    "#### We build triplets (anchor, positive, negative) and split into train (70%), validation (20%), and test (10%) sets.\n",
    "\n",
    "> #### Total triplets: ~323,000.\n",
    "\n",
    "\n",
    "## Senetence Transformer Architecture\n",
    "\n",
    "### The sentence transformer has a minimalist architecture.\n",
    "\n",
    "> Embedding Layer: Converts token IDs to dense vectors.\n",
    "\n",
    "> Sinusoidal Positional Encoding: Adds positional information without training.\n",
    "\n",
    "> Transformer Encoder: Stacks num_layers layers of basic self-attention blocks.\n",
    "\n",
    "> CLS Pooling: Extracts the representation of the [CLS] token (first token). Can be used for training Classification Tasks Downstream.\n",
    "\n",
    "> Projection Head: Represents the learned Transformer Encoder using a MLP layer.\n",
    "\n",
    "\n",
    "## Pretraining Architecture\n",
    "\n",
    "### We pretrain the sentence transformer using triplet loss.\n",
    "\n",
    "> Inputs: Each batch contains (anchor, positive, negative) sentence triplets.\n",
    "\n",
    "> Forward pass: Encode anchor, positive, and negative independently and Get their embeddings.\n",
    "\n",
    "> Minimize triplet loss to pull positive pairs closer and push negative pairs farther apart in embedding space.\n",
    "\n",
    "## Training enhancements:\n",
    "\n",
    "> Mixed Precision (AMP) for faster and memory-efficient training.\n",
    "\n",
    "> Gradient Clipping to avoid exploding gradients.\n",
    "\n",
    "> Cosine Annealing Scheduler to gradually reduce the learning rate.\n",
    "\n",
    "> Early Stopping based on validation loss.\n",
    "\n",
    "## Inference\n",
    "> Compute similarity of anchor-positive sentences.\n",
    "> Compute similarity of anchor-negative sentences.\n",
    "\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "* Sentence Transformer is defined in `sent_transformer.py` because we will be calling it for Multi-Task Expansion\n",
    "* Lightweight pretraining dataset and architecture, so maximal accuracy cannot be acheived on downstream tasks.\n",
    "* Positional encodings are fixed, not learned.\n",
    "* All attention operations are standard PyTorch Transformer blocks.\n",
    "* Further reading:\n",
    "  \n",
    "  * Attention Is All You Need: https://arxiv.org/abs/1706.03762\n",
    "  * Sentence-BERT: https://arxiv.org/abs/1908.10084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2c83b-ebdc-429f-aafc-3a73a31be40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load and merge STSB train + validation datasets\n",
    "sts = load_dataset(\"sentence-transformers/stsb\")\n",
    "sts_trainval = concatenate_datasets([sts[\"train\"], sts[\"validation\"]])\n",
    "\n",
    "\n",
    "# Create associated sentences map\n",
    "pairs = defaultdict(list)\n",
    "for row in sts_trainval:\n",
    "    s1, s2, score = row[\"sentence1\"], row[\"sentence2\"], row[\"score\"]\n",
    "    pairs[s1].append((s2, score))\n",
    "    pairs[s2].append((s1, score))\n",
    "\n",
    "# Create a sentence pool to randomly sample negatives.\n",
    "sent_pool = list(set(sts_trainval[\"sentence1\"] + sts_trainval[\"sentence2\"]))\n",
    "\n",
    "# Set thresholds for positive, negative, and num triplets for each anchor \n",
    "POS_T, NEG_T, K = 0.6, 0.5, 3\n",
    "\n",
    "# Generate triplets for STSB dataset by randomly sampling extra negatives from sent_pool.\n",
    "all_triplets = set()\n",
    "for anchor, lst in tqdm(pairs.items(), desc=\"STSB trainval triplets\"):\n",
    "    pos = [s for s, sc in lst if sc >= POS_T]\n",
    "    neg = [s for s, sc in lst if sc <= NEG_T]\n",
    "    if not pos:\n",
    "        continue\n",
    "    for p in pos:\n",
    "        for _ in range(K):\n",
    "            if neg:\n",
    "                n = random.choice(neg)\n",
    "            else:\n",
    "                n = random.choice(sent_pool)\n",
    "                while n in (anchor, p):\n",
    "                    n = random.choice(sent_pool)\n",
    "            all_triplets.add((anchor, p, n))\n",
    "\n",
    "\n",
    "# Load QQP_triplets dataset\n",
    "qqp = load_dataset(\"embedding-data/QQP_triplets\")\n",
    "for split in qqp:\n",
    "    for row in tqdm(qqp[split], desc=f\"QQP {split}\"):\n",
    "        query = row[\"set\"][\"query\"]\n",
    "        positives = row[\"set\"][\"pos\"]\n",
    "        negatives = row[\"set\"][\"neg\"]\n",
    "        for p in positives:\n",
    "            for n in negatives:\n",
    "                all_triplets.add((query, p, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c67f91-adda-45c8-815d-fc8db37497fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "all_triplets = list(all_triplets)\n",
    "random.seed(42)\n",
    "random.shuffle(all_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33f739-e613-4add-8717-8d6b019e7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "teacher_name = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "device       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_name)\n",
    "teacher_model     = AutoModel.from_pretrained(teacher_name).eval().to(device)\n",
    "\n",
    "all_texts = set()\n",
    "for a, p, n in all_triplets:\n",
    "    all_texts.add(a); all_texts.add(p); all_texts.add(n)\n",
    "all_texts = list(all_texts)\n",
    "\n",
    "teacher_embeds = {}\n",
    "batch_size = 512\n",
    "for i in tqdm(range(0, len(all_texts), batch_size), desc=\"Embedding teacher\"):\n",
    "    batch_txt = all_texts[i:i+batch_size]\n",
    "    enc = teacher_tokenizer(\n",
    "        batch_txt,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = teacher_model(**enc)\n",
    "        embs = out.pooler_output.cpu()   # (B, hidden_dim)\n",
    "    for text, emb in zip(batch_txt, embs):\n",
    "        teacher_embeds[text] = emb\n",
    "\n",
    "torch.save(teacher_embeds, \"teacher_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7473c6d-85fc-4b85-afaf-2166b7f539c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/20/10 Train/Val/Test split\n",
    "n = len(all_triplets)\n",
    "n_train = int(0.7 * n)\n",
    "n_val   = int(0.2 * n)\n",
    "\n",
    "train_data = all_triplets[:n_train]\n",
    "val_data   = all_triplets[n_train : n_train + n_val]\n",
    "test_data  = all_triplets[n_train + n_val : ]\n",
    "\n",
    "\n",
    "print(f\"Total triplets: {n}\")\n",
    "print(f\" Train: {len(train_data)}\")\n",
    "print(f\" Val:   {len(val_data)}\")\n",
    "print(f\" Test:  {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d6336-0b5b-4ade-a46b-93922b6fcd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, triplets, tokenizer, max_len=MAX_LEN):\n",
    "        self.triplets = triplets\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a,p,n = self.triplets[idx]\n",
    "        return [(self.tok.tokenize(s), teacher_embeds[s]) for s in (a,p,n)]\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    anchors, positives, negatives = zip(*batch)\n",
    "    a_seqs,  t_as = zip(*anchors)\n",
    "    p_seqs,  t_ps = zip(*positives)\n",
    "    n_seqs,  t_ns = zip(*negatives)\n",
    "\n",
    "    def pad_and_mask(seq_tensors):\n",
    "        truncated = [s[:MAX_LEN] for s in seq_tensors]\n",
    "        ids = pad_sequence(truncated, batch_first=True, padding_value=PAD_ID)\n",
    "        if ids.size(1) > MAX_LEN:\n",
    "            ids = ids[:, :MAX_LEN]\n",
    "        mask = ids.ne(PAD_ID).long()\n",
    "        return ids, mask\n",
    "\n",
    "    a_ids, a_mask = pad_and_mask(a_seqs)\n",
    "    p_ids, p_mask = pad_and_mask(p_seqs)\n",
    "    n_ids, n_mask = pad_and_mask(n_seqs)\n",
    "\n",
    "    t_a = torch.stack(t_as, dim=0)\n",
    "    t_p = torch.stack(t_ps, dim=0)\n",
    "    t_n = torch.stack(t_ns, dim=0)\n",
    "\n",
    "    return (a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask), t_a, t_p, t_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9f417-dc1e-4b8e-bafc-edfe7df2a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\"bpe_merged.json\")\n",
    "train_dataset = TripletDataset(train_data, tokenizer)\n",
    "val_dataset = TripletDataset(val_data, tokenizer)\n",
    "test_dataset   = TripletDataset(test_data, tokenizer)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TripletDataset(train_data, tokenizer),\n",
    "    batch_size=16, shuffle=True,\n",
    "    num_workers=0, pin_memory=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TripletDataset(val_data, tokenizer),\n",
    "    batch_size=16, shuffle=False,\n",
    "    num_workers=0, pin_memory=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TripletDataset(test_data, tokenizer),\n",
    "    batch_size=16, shuffle=False,\n",
    "    num_workers=0, pin_memory=True,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e87c0c-9012-4ba2-a715-7a862dc778aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enums import D_MODEL, NHEAD, NUM_LAYERS, PROJ_DIM, DROPOUT, MARGIN\n",
    "\n",
    "\n",
    "class PretrainSentenceTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 d_model=D_MODEL,\n",
    "                 nhead=NHEAD,\n",
    "                 num_layers=NUM_LAYERS,\n",
    "                 proj_dim=PROJ_DIM,\n",
    "                 margin=MARGIN,\n",
    "                 dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = SentenceTransformer(\n",
    "            vocab_size, d_model, nhead, num_layers, proj_dim, dropout\n",
    "        )\n",
    "        # cosine-distance triplet loss\n",
    "        self.cos = nn.CosineSimilarity(dim=1)\n",
    "        self.triplet = nn.TripletMarginWithDistanceLoss(\n",
    "            distance_function=lambda x, y: 1 - self.cos(x, y),\n",
    "            margin=margin\n",
    "        )\n",
    "    \n",
    "    def forward(self, a, a_mask, p, p_mask, n, n_mask):\n",
    "        cls_a, e_a = self.encoder(a, attention_mask=a_mask, return_all=True)\n",
    "        cls_p, e_p = self.encoder(p, attention_mask=p_mask, return_all=True)\n",
    "        cls_n, e_n = self.encoder(n, attention_mask=n_mask, return_all=True)\n",
    "        return cls_a, e_a, cls_p, e_p, cls_n, e_n\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        (a, a_mask), (p, p_mask), (n, n_mask), t_a, t_p, t_n = batch\n",
    "        cls_a, e_a, cls_p, e_p, cls_n, e_n = self(a, a_mask, p, p_mask, n, n_mask)\n",
    "\n",
    "        loss_trip  = self.triplet(cls_a, cls_p, cls_n)\n",
    "\n",
    "        loss_da = (1 - self.cos(cls_a, t_a)).mean()\n",
    "        loss_dp = (1 - self.cos(cls_p, t_p)).mean()\n",
    "        loss_dn = (1 - self.cos(cls_n, t_n)).mean()\n",
    "\n",
    "        return loss_trip + loss_da + loss_dp + loss_dn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbd16d-45d0-41fa-8499-c5c8628d7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = max(tokenizer.rev_merged.keys()) + 3\n",
    "\n",
    "model     = PretrainSentenceTransformer(vocab_size).to(device)\n",
    "opt       = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs    = 50\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "\n",
    "# AMP setup\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "\n",
    "# Early stopping & checkpointing params\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience      = 3\n",
    "no_improve    = 0\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch} Train\"):\n",
    "        (a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask), t_as, t_ps, t_ns = batch\n",
    "        a_ids, a_mask = a_ids.to(device), a_mask.to(device)\n",
    "        p_ids, p_mask = p_ids.to(device), p_mask.to(device)\n",
    "        n_ids, n_mask = n_ids.to(device), n_mask.to(device)\n",
    "        t_as, t_ps, t_ns = t_as.to(device), t_ps.to(device), t_ns.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                loss = model.training_step(((a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask), t_as, t_ps, t_ns))\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss = model.training_step(((a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask), t_as, t_ps, t_ns))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch} Val\"):\n",
    "            (a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask), t_as, t_ps, t_ns = batch\n",
    "            a_ids, a_mask = a_ids.to(device), a_mask.to(device)\n",
    "            p_ids, p_mask = p_ids.to(device), p_mask.to(device)\n",
    "            n_ids, n_mask = n_ids.to(device), n_mask.to(device)\n",
    "            t_as, t_ps, t_ns = t_as.to(device), t_ps.to(device), t_ns.to(device)\n",
    "\n",
    "            if use_amp:\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                    loss = model.training_step(((a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask), t_as, t_ps, t_ns))\n",
    "            else:\n",
    "                loss = model.training_step(((a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask), t_as, t_ps, t_ns))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Learning Rate Schedule\n",
    "    scheduler.step()\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    print(f\"Epoch {epoch}/{epochs} — \"\n",
    "          f\"train_loss={train_loss:.5f}  val_loss={val_loss:.5f}  lr={lr:.1e}\")\n",
    "\n",
    "    # Checkpoint after each epoch.\n",
    "    # Notice we are only checkpointing the model encoder as it is is the sentence transformer.\n",
    "    enc_state = model.encoder.state_dict()\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'encoder_state_dict': enc_state,\n",
    "        'optimizer_state_dict': opt.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, os.path.join(checkpoint_dir, f\"encoder_epoch_{epoch}.pt\"))\n",
    "\n",
    "    torch.save(enc_state, os.path.join(checkpoint_dir, f\"epoch_{epoch}.pt\"))\n",
    "\n",
    "    # Early stopping to avoid overfitting\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        torch.save(model.encoder.state_dict(), \"best_encoder.pt\")\n",
    "        print(\"  ↳ New best Sentence Transformer saved.\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print(f\"  ↳ No improvement for {no_improve} epoch(s).\")\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Stopping early after {patience} epochs without improvement.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e8d46f-b46b-4aca-97bf-066a3160d8e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check on test data.\n",
    "# Notice the positive sentences have a greater similarity than the negative sentances when compared to the anchor sentences.\n",
    "for idx in np.random.randint(1,100,10):\n",
    "    (a, a_mask), (p, p_mask), (n, n_mask) = collate([test_dataset[idx]])\n",
    "\n",
    "    a_ids, a_mask = a.to(device), a_mask.to(device)\n",
    "    p_ids, p_mask = p.to(device), p_mask.to(device)\n",
    "    n_ids, n_mask = n.to(device), n_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        e_a, e_p, e_n = model(a, a_mask, p, p_mask, n, n_mask)\n",
    "        cos = F.cosine_similarity(e_a, e_p).item()\n",
    "        cos_neg = F.cosine_similarity(e_a, e_n).item()\n",
    "    \n",
    "    print(f\"cos(+): {cos:.3f}  cos(-): {cos_neg:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882bd9c9-31cb-456b-8243-686f9e84cee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
