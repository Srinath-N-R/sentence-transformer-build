{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0d2bd3-a0ce-47ea-a384-13a55882a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tokenizer import Tokenizer\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from enums import PAD_ID, MAX_LEN, CLS_ID\n",
    "from sent_transformer import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9a9c7-413b-438b-b17a-a15ec62e9ea3",
   "metadata": {},
   "source": [
    "# Senetence Transformer Overview\n",
    "## Datasets:\n",
    "* STS-B (Semantic Textual Similarity Benchmark)\n",
    "  \n",
    "> Sentences are paired with similarity scores. We use high-score pairs as positives (score ≥ 0.6) and low-score pairs as negatives (score ≤ 0.5).\n",
    "* QQP (Quora Question Pairs)\n",
    "\n",
    "> Provided triplet dataset with queries, positive matches, and negative examples.\n",
    "    \n",
    "\n",
    "#### The two datasets are merged to create a diverse training pool.\n",
    "#### We build triplets (anchor, positive, negative) and split into train (70%), validation (20%), and test (10%) sets.\n",
    "\n",
    "> #### Total triplets: ~323,000.\n",
    "\n",
    "\n",
    "## Senetence Transformer Architecture\n",
    "\n",
    "### The sentence transformer has a minimalist architecture.\n",
    "\n",
    "> Embedding Layer: Converts token IDs to dense vectors.\n",
    "\n",
    "> Sinusoidal Positional Encoding: Adds positional information without training.\n",
    "\n",
    "> Transformer Encoder: Stacks num_layers layers of basic self-attention blocks.\n",
    "\n",
    "> CLS Pooling: Extracts the representation of the [CLS] token (first token). Can be used for training Classification Tasks Downstream.\n",
    "\n",
    "> Projection Head: Represents the learned Transformer Encoder using a MLP layer.\n",
    "\n",
    "\n",
    "## Pretraining Architecture\n",
    "\n",
    "### We pretrain the sentence transformer using triplet loss.\n",
    "\n",
    "> Inputs: Each batch contains (anchor, positive, negative) sentence triplets.\n",
    "\n",
    "> Forward pass: Encode anchor, positive, and negative independently and Get their embeddings.\n",
    "\n",
    "> Minimize triplet loss to pull positive pairs closer and push negative pairs farther apart in embedding space.\n",
    "\n",
    "## Training enhancements:\n",
    "\n",
    "> Mixed Precision (AMP) for faster and memory-efficient training.\n",
    "\n",
    "> Gradient Clipping to avoid exploding gradients.\n",
    "\n",
    "> Cosine Annealing Scheduler to gradually reduce the learning rate.\n",
    "\n",
    "> Early Stopping based on validation loss.\n",
    "\n",
    "## Inference\n",
    "> Compute similarity of anchor-positive sentences.\n",
    "> Compute similarity of anchor-negative sentences.\n",
    "\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "* Sentence Transformer is defined in `sent_transformer.py` because we will be calling it for Multi-Task Expansion\n",
    "* Lightweight pretraining dataset and architecture, so maximal accuracy cannot be acheived on downstream tasks.\n",
    "* Positional encodings are fixed, not learned.\n",
    "* All attention operations are standard PyTorch Transformer blocks.\n",
    "* Further reading:\n",
    "  \n",
    "  * Attention Is All You Need: https://arxiv.org/abs/1706.03762\n",
    "  * Sentence-BERT: https://arxiv.org/abs/1908.10084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc2c83b-ebdc-429f-aafc-3a73a31be40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STS trainval triplets: 100%|██████████| 13227/13227 [00:00<00:00, 839851.33it/s]\n",
      "QQP train: 100%|█████████████████████| 101762/101762 [00:02<00:00, 42287.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triplets: 323435\n",
      " Train: 226404\n",
      " Val:   64687\n",
      " Test:  32344\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load and merge STSB train + validation datasets\n",
    "sts = load_dataset(\"sentence-transformers/stsb\")\n",
    "sts_trainval = concatenate_datasets([sts[\"train\"], sts[\"validation\"]])\n",
    "\n",
    "\n",
    "# Create associated sentences map\n",
    "pairs = defaultdict(list)\n",
    "for row in sts_trainval:\n",
    "    s1, s2, score = row[\"sentence1\"], row[\"sentence2\"], row[\"score\"]\n",
    "    pairs[s1].append((s2, score))\n",
    "    pairs[s2].append((s1, score))\n",
    "\n",
    "# Create a sentence pool to randomly sample negatives.\n",
    "sent_pool = list(set(sts_trainval[\"sentence1\"] + sts_trainval[\"sentence2\"]))\n",
    "\n",
    "# Set thresholds for positive, negative, and num triplets for each anchor \n",
    "POS_T, NEG_T, K = 0.6, 0.5, 3\n",
    "\n",
    "# Generate triplets for STSB dataset by randomly sampling extra negatives from sent_pool.\n",
    "all_triplets = set()\n",
    "for anchor, lst in tqdm(pairs.items(), desc=\"STSB trainval triplets\"):\n",
    "    pos = [s for s, sc in lst if sc >= POS_T]\n",
    "    neg = [s for s, sc in lst if sc <= NEG_T]\n",
    "    if not pos:\n",
    "        continue\n",
    "    for p in pos:\n",
    "        for _ in range(K):\n",
    "            if neg:\n",
    "                n = random.choice(neg)\n",
    "            else:\n",
    "                n = random.choice(sent_pool)\n",
    "                while n in (anchor, p):\n",
    "                    n = random.choice(sent_pool)\n",
    "            all_triplets.add((anchor, p, n))\n",
    "\n",
    "\n",
    "# Load QQP_triplets dataset\n",
    "qqp = load_dataset(\"embedding-data/QQP_triplets\")\n",
    "for split in qqp:\n",
    "    for row in tqdm(qqp[split], desc=f\"QQP {split}\"):\n",
    "        query = row[\"set\"][\"query\"]\n",
    "        positives = row[\"set\"][\"pos\"]\n",
    "        negatives = row[\"set\"][\"neg\"]\n",
    "        for p in positives:\n",
    "            for n in negatives:\n",
    "                all_triplets.add((query, p, n))\n",
    "\n",
    "# Shuffle\n",
    "all_triplets = list(all_triplets)\n",
    "random.seed(42)\n",
    "random.shuffle(all_triplets)\n",
    "\n",
    "# 70/20/10 Train/Val/Test split\n",
    "n = len(all_triplets)\n",
    "n_train = int(0.7 * n)\n",
    "n_val   = int(0.2 * n)\n",
    "\n",
    "train_data = all_triplets[:n_train]\n",
    "val_data   = all_triplets[n_train : n_train + n_val]\n",
    "test_data  = all_triplets[n_train + n_val : ]\n",
    "\n",
    "\n",
    "print(f\"Total triplets: {n}\")\n",
    "print(f\" Train: {len(train_data)}\")\n",
    "print(f\" Val:   {len(val_data)}\")\n",
    "print(f\" Test:  {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812d6336-0b5b-4ade-a46b-93922b6fcd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, triplets, tokenizer, max_len=MAX_LEN):\n",
    "        self.triplets = triplets\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a,p,n = self.triplets[idx]\n",
    "        return [self.tok.tokenize(s).tolist() for s in (a,p,n)]\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    a, p, n = zip(*batch)\n",
    "\n",
    "    def pad(seq):\n",
    "        L = max(len(s) for s in seq)\n",
    "        batch_ids = [s + [PAD_ID] * (L - len(s)) for s in seq]\n",
    "        attn_mask = [[1] * len(s) + [0] * (L - len(s)) for s in seq]\n",
    "        return (\n",
    "            torch.tensor(batch_ids, dtype=torch.long),\n",
    "            torch.tensor(attn_mask, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    a_ids, a_mask = pad(a)\n",
    "    p_ids, p_mask = pad(p)\n",
    "    n_ids, n_mask = pad(n)\n",
    "\n",
    "    return (a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c9f417-dc1e-4b8e-bafc-edfe7df2a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\"bpe_merged.json\")\n",
    "train_dataset = TripletDataset(train_data, tokenizer)\n",
    "val_dataset = TripletDataset(val_data, tokenizer)\n",
    "test_dataset   = TripletDataset(test_data, tokenizer)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TripletDataset(train_data, tokenizer),\n",
    "    batch_size=32, shuffle=True,\n",
    "    num_workers=0, pin_memory=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TripletDataset(val_data, tokenizer),\n",
    "    batch_size=32, shuffle=False,\n",
    "    num_workers=0, pin_memory=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TripletDataset(test_data, tokenizer),\n",
    "    batch_size=32, shuffle=False,\n",
    "    num_workers=0, pin_memory=True,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e87c0c-9012-4ba2-a715-7a862dc778aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enums import D_MODEL, NHEAD, NUM_LAYERS, PROJ_DIM, DROPOUT, MARGIN\n",
    "\n",
    "\n",
    "class PretrainSentenceTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 d_model=D_MODEL,\n",
    "                 nhead=NHEAD,\n",
    "                 num_layers=NUM_LAYERS,\n",
    "                 proj_dim=PROJ_DIM,\n",
    "                 margin=MARGIN,\n",
    "                 dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = SentenceTransformer(\n",
    "            vocab_size, d_model, nhead, num_layers, proj_dim, dropout\n",
    "        )\n",
    "        # cosine-distance triplet loss\n",
    "        cos_sim = nn.CosineSimilarity(dim=1)\n",
    "        self.loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "            distance_function=lambda x, y: 1 - cos_sim(x, y),\n",
    "            margin=margin\n",
    "        )\n",
    "\n",
    "    def forward(self, a, a_mask, p, p_mask, n, n_mask):\n",
    "        e_a = self.encoder(a, attention_mask=a_mask)\n",
    "        e_p = self.encoder(p, attention_mask=p_mask)\n",
    "        e_n = self.encoder(n, attention_mask=n_mask)\n",
    "        return e_a, e_p, e_n\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        (a, a_mask), (p, p_mask), (n, n_mask) = batch\n",
    "        e_a, e_p, e_n = self(a, a_mask, p, p_mask, n, n_mask)\n",
    "        return self.loss_fn(e_a, e_p, e_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8dbd16d-45d0-41fa-8499-c5c8628d7529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train: 100%|████████████████████████| 7076/7076 [38:52<00:00,  3.03it/s]\n",
      "Epoch 1 Val:   0%|                                     | 0/2022 [00:00<?, ?it/s]/Users/srinathramalingam/miniconda3/envs/sent-transform/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Epoch 1 Val: 100%|██████████████████████████| 2022/2022 [04:24<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 — train_loss=0.05900  val_loss=0.04115  lr=1.0e-03\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train: 100%|████████████████████████| 7076/7076 [39:03<00:00,  3.02it/s]\n",
      "Epoch 2 Val: 100%|██████████████████████████| 2022/2022 [04:22<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 — train_loss=0.04117  val_loss=0.03261  lr=1.0e-03\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train: 100%|████████████████████████| 7076/7076 [43:46<00:00,  2.69it/s]\n",
      "Epoch 3 Val: 100%|██████████████████████████| 2022/2022 [05:48<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 — train_loss=0.03570  val_loss=0.03046  lr=9.9e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train: 100%|████████████████████████| 7076/7076 [40:09<00:00,  2.94it/s]\n",
      "Epoch 4 Val: 100%|██████████████████████████| 2022/2022 [04:26<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 — train_loss=0.03216  val_loss=0.02902  lr=9.8e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train: 100%|████████████████████████| 7076/7076 [38:32<00:00,  3.06it/s]\n",
      "Epoch 5 Val: 100%|██████████████████████████| 2022/2022 [04:22<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 — train_loss=0.02954  val_loss=0.02693  lr=9.8e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train: 100%|████████████████████████| 7076/7076 [38:23<00:00,  3.07it/s]\n",
      "Epoch 6 Val: 100%|██████████████████████████| 2022/2022 [04:20<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 — train_loss=0.02720  val_loss=0.02508  lr=9.6e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train: 100%|████████████████████████| 7076/7076 [38:14<00:00,  3.08it/s]\n",
      "Epoch 7 Val: 100%|██████████████████████████| 2022/2022 [04:20<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 — train_loss=0.02587  val_loss=0.02257  lr=9.5e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train: 100%|████████████████████████| 7076/7076 [38:28<00:00,  3.07it/s]\n",
      "Epoch 8 Val: 100%|██████████████████████████| 2022/2022 [04:31<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 — train_loss=0.02393  val_loss=0.02203  lr=9.4e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train: 100%|████████████████████████| 7076/7076 [38:12<00:00,  3.09it/s]\n",
      "Epoch 9 Val: 100%|██████████████████████████| 2022/2022 [04:19<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 — train_loss=0.02259  val_loss=0.02184  lr=9.2e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train: 100%|███████████████████████| 7076/7076 [38:00<00:00,  3.10it/s]\n",
      "Epoch 10 Val: 100%|█████████████████████████| 2022/2022 [04:30<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 — train_loss=0.02133  val_loss=0.01982  lr=9.0e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train: 100%|███████████████████████| 7076/7076 [38:03<00:00,  3.10it/s]\n",
      "Epoch 11 Val: 100%|█████████████████████████| 2022/2022 [04:23<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 — train_loss=0.02016  val_loss=0.01929  lr=8.9e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train: 100%|███████████████████████| 7076/7076 [37:47<00:00,  3.12it/s]\n",
      "Epoch 12 Val: 100%|█████████████████████████| 2022/2022 [04:20<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 — train_loss=0.01902  val_loss=0.01957  lr=8.6e-04\n",
      "  ↳ No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train: 100%|███████████████████████| 7076/7076 [38:00<00:00,  3.10it/s]\n",
      "Epoch 13 Val: 100%|█████████████████████████| 2022/2022 [04:28<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 — train_loss=0.01801  val_loss=0.01860  lr=8.4e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train: 100%|███████████████████████| 7076/7076 [38:23<00:00,  3.07it/s]\n",
      "Epoch 14 Val: 100%|█████████████████████████| 2022/2022 [04:26<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 — train_loss=0.01711  val_loss=0.01724  lr=8.2e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train: 100%|███████████████████████| 7076/7076 [38:29<00:00,  3.06it/s]\n",
      "Epoch 15 Val: 100%|█████████████████████████| 2022/2022 [04:18<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 — train_loss=0.01635  val_loss=0.01648  lr=7.9e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Train: 100%|███████████████████████| 7076/7076 [38:56<00:00,  3.03it/s]\n",
      "Epoch 16 Val: 100%|█████████████████████████| 2022/2022 [04:35<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 — train_loss=0.01542  val_loss=0.01631  lr=7.7e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Train: 100%|███████████████████████| 7076/7076 [38:51<00:00,  3.03it/s]\n",
      "Epoch 17 Val: 100%|█████████████████████████| 2022/2022 [04:27<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 — train_loss=0.01477  val_loss=0.01574  lr=7.4e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Train: 100%|███████████████████████| 7076/7076 [38:22<00:00,  3.07it/s]\n",
      "Epoch 18 Val: 100%|█████████████████████████| 2022/2022 [04:27<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 — train_loss=0.01409  val_loss=0.01615  lr=7.1e-04\n",
      "  ↳ No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Train: 100%|███████████████████████| 7076/7076 [45:43<00:00,  2.58it/s]\n",
      "Epoch 19 Val: 100%|█████████████████████████| 2022/2022 [04:40<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 — train_loss=0.01350  val_loss=0.01524  lr=6.8e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Train: 100%|███████████████████████| 7076/7076 [48:35<00:00,  2.43it/s]\n",
      "Epoch 20 Val: 100%|█████████████████████████| 2022/2022 [04:37<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 — train_loss=0.01269  val_loss=0.01463  lr=6.5e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 Train: 100%|███████████████████████| 7076/7076 [43:25<00:00,  2.72it/s]\n",
      "Epoch 21 Val: 100%|█████████████████████████| 2022/2022 [06:13<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 — train_loss=0.01203  val_loss=0.01432  lr=6.2e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 Train: 100%|███████████████████████| 7076/7076 [48:49<00:00,  2.42it/s]\n",
      "Epoch 22 Val: 100%|█████████████████████████| 2022/2022 [06:30<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 — train_loss=0.01149  val_loss=0.01405  lr=5.9e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 Train: 100%|███████████████████████| 7076/7076 [50:07<00:00,  2.35it/s]\n",
      "Epoch 23 Val: 100%|█████████████████████████| 2022/2022 [06:26<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 — train_loss=0.01085  val_loss=0.01387  lr=5.6e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 Train: 100%|███████████████████████| 7076/7076 [51:59<00:00,  2.27it/s]\n",
      "Epoch 24 Val: 100%|█████████████████████████| 2022/2022 [06:31<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 — train_loss=0.01022  val_loss=0.01327  lr=5.3e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 Train: 100%|███████████████████████| 7076/7076 [52:28<00:00,  2.25it/s]\n",
      "Epoch 25 Val: 100%|█████████████████████████| 2022/2022 [06:08<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 — train_loss=0.00982  val_loss=0.01328  lr=5.0e-04\n",
      "  ↳ No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 Train: 100%|███████████████████████| 7076/7076 [48:11<00:00,  2.45it/s]\n",
      "Epoch 26 Val: 100%|█████████████████████████| 2022/2022 [06:31<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 — train_loss=0.00913  val_loss=0.01254  lr=4.7e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 Train: 100%|███████████████████████| 7076/7076 [44:14<00:00,  2.67it/s]\n",
      "Epoch 27 Val: 100%|█████████████████████████| 2022/2022 [04:38<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 — train_loss=0.00868  val_loss=0.01251  lr=4.4e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 Train: 100%|███████████████████████| 7076/7076 [46:43<00:00,  2.52it/s]\n",
      "Epoch 28 Val: 100%|█████████████████████████| 2022/2022 [07:53<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 — train_loss=0.00824  val_loss=0.01233  lr=4.1e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 Train: 100%|███████████████████████| 7076/7076 [58:18<00:00,  2.02it/s]\n",
      "Epoch 29 Val: 100%|█████████████████████████| 2022/2022 [07:08<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 — train_loss=0.00784  val_loss=0.01198  lr=3.8e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 Train: 100%|███████████████████████| 7076/7076 [58:48<00:00,  2.01it/s]\n",
      "Epoch 30 Val: 100%|█████████████████████████| 2022/2022 [06:04<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 — train_loss=0.00734  val_loss=0.01137  lr=3.5e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31 Train: 100%|███████████████████████| 7076/7076 [45:47<00:00,  2.58it/s]\n",
      "Epoch 31 Val: 100%|█████████████████████████| 2022/2022 [04:39<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 — train_loss=0.00688  val_loss=0.01147  lr=3.2e-04\n",
      "  ↳ No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 Train: 100%|███████████████████████| 7076/7076 [39:44<00:00,  2.97it/s]\n",
      "Epoch 32 Val: 100%|█████████████████████████| 2022/2022 [04:38<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 — train_loss=0.00655  val_loss=0.01112  lr=2.9e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33 Train: 100%|███████████████████████| 7076/7076 [42:07<00:00,  2.80it/s]\n",
      "Epoch 33 Val: 100%|█████████████████████████| 2022/2022 [05:29<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 — train_loss=0.00622  val_loss=0.01088  lr=2.6e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 Train: 100%|███████████████████████| 7076/7076 [45:25<00:00,  2.60it/s]\n",
      "Epoch 34 Val: 100%|█████████████████████████| 2022/2022 [05:31<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 — train_loss=0.00592  val_loss=0.01075  lr=2.3e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 Train: 100%|███████████████████████| 7076/7076 [45:19<00:00,  2.60it/s]\n",
      "Epoch 35 Val: 100%|█████████████████████████| 2022/2022 [05:43<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 — train_loss=0.00551  val_loss=0.01059  lr=2.1e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 Train: 100%|███████████████████████| 7076/7076 [43:00<00:00,  2.74it/s]\n",
      "Epoch 36 Val: 100%|█████████████████████████| 2022/2022 [04:37<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 — train_loss=0.00519  val_loss=0.01039  lr=1.8e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37 Train: 100%|███████████████████████| 7076/7076 [39:48<00:00,  2.96it/s]\n",
      "Epoch 37 Val: 100%|█████████████████████████| 2022/2022 [04:39<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 — train_loss=0.00504  val_loss=0.01024  lr=1.6e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 Train: 100%|███████████████████████| 7076/7076 [39:45<00:00,  2.97it/s]\n",
      "Epoch 38 Val: 100%|█████████████████████████| 2022/2022 [04:20<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 — train_loss=0.00474  val_loss=0.01012  lr=1.4e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 Train: 100%|███████████████████████| 7076/7076 [39:40<00:00,  2.97it/s]\n",
      "Epoch 39 Val: 100%|█████████████████████████| 2022/2022 [04:37<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 — train_loss=0.00455  val_loss=0.00999  lr=1.1e-04\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 Train: 100%|███████████████████████| 7076/7076 [39:40<00:00,  2.97it/s]\n",
      "Epoch 40 Val: 100%|█████████████████████████| 2022/2022 [04:37<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 — train_loss=0.00433  val_loss=0.00985  lr=9.5e-05\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41 Train: 100%|███████████████████████| 7076/7076 [39:15<00:00,  3.00it/s]\n",
      "Epoch 41 Val: 100%|█████████████████████████| 2022/2022 [04:34<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 — train_loss=0.00412  val_loss=0.00975  lr=7.8e-05\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42 Train: 100%|███████████████████████| 7076/7076 [39:24<00:00,  2.99it/s]\n",
      "Epoch 42 Val: 100%|█████████████████████████| 2022/2022 [04:36<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 — train_loss=0.00391  val_loss=0.00969  lr=6.2e-05\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43 Train: 100%|███████████████████████| 7076/7076 [39:23<00:00,  2.99it/s]\n",
      "Epoch 43 Val: 100%|█████████████████████████| 2022/2022 [04:34<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 — train_loss=0.00381  val_loss=0.00968  lr=4.8e-05\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44 Train: 100%|███████████████████████| 7076/7076 [39:07<00:00,  3.01it/s]\n",
      "Epoch 44 Val: 100%|█████████████████████████| 2022/2022 [04:36<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 — train_loss=0.00373  val_loss=0.00949  lr=3.5e-05\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45 Train: 100%|███████████████████████| 7076/7076 [39:16<00:00,  3.00it/s]\n",
      "Epoch 45 Val: 100%|█████████████████████████| 2022/2022 [04:34<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 — train_loss=0.00358  val_loss=0.00946  lr=2.4e-05\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46 Train: 100%|███████████████████████| 7076/7076 [39:20<00:00,  3.00it/s]\n",
      "Epoch 46 Val: 100%|█████████████████████████| 2022/2022 [04:30<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 — train_loss=0.00347  val_loss=0.00945  lr=1.6e-05\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47 Train: 100%|███████████████████████| 7076/7076 [39:01<00:00,  3.02it/s]\n",
      "Epoch 47 Val: 100%|█████████████████████████| 2022/2022 [04:37<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 — train_loss=0.00339  val_loss=0.00943  lr=8.9e-06\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48 Train: 100%|███████████████████████| 7076/7076 [39:19<00:00,  3.00it/s]\n",
      "Epoch 48 Val: 100%|█████████████████████████| 2022/2022 [04:33<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 — train_loss=0.00339  val_loss=0.00940  lr=3.9e-06\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 Train: 100%|███████████████████████| 7076/7076 [38:57<00:00,  3.03it/s]\n",
      "Epoch 49 Val: 100%|█████████████████████████| 2022/2022 [04:37<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 — train_loss=0.00345  val_loss=0.00940  lr=9.9e-07\n",
      "  ↳ No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50 Train: 100%|███████████████████████| 7076/7076 [39:33<00:00,  2.98it/s]\n",
      "Epoch 50 Val: 100%|█████████████████████████| 2022/2022 [04:38<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 — train_loss=0.00331  val_loss=0.00940  lr=0.0e+00\n",
      "  ↳ New best Sentence Transformer saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = max(tokenizer.rev_merged.keys()) + 3\n",
    "\n",
    "model     = PretrainSentenceTransformer(vocab_size).to(device)\n",
    "opt       = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs    = 50\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "\n",
    "# AMP setup\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler  = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "\n",
    "# Early stopping & checkpointing params\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience      = 3\n",
    "no_improve    = 0\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch} Train\"):\n",
    "        # unpack tuples\n",
    "        (a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask) = batch\n",
    "    \n",
    "        # move each tensor explicitly to device\n",
    "        a_ids, a_mask = a_ids.to(device), a_mask.to(device)\n",
    "        p_ids, p_mask = p_ids.to(device), p_mask.to(device)\n",
    "        n_ids, n_mask = n_ids.to(device), n_mask.to(device)\n",
    "    \n",
    "        if use_amp:\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                loss = model.training_step(((a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask)))\n",
    "            opt.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss = model.training_step(((a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask)))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "    \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch} Val\"):\n",
    "            (a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask) = batch\n",
    "            a_ids, a_mask = a_ids.to(device), a_mask.to(device)\n",
    "            p_ids, p_mask = p_ids.to(device), p_mask.to(device)\n",
    "            n_ids, n_mask = n_ids.to(device), n_mask.to(device)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                if use_amp:\n",
    "                    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                        loss = model.training_step(((a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask)))\n",
    "                else:\n",
    "                    loss = model.training_step(((a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask)))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Learning Rate Schedule\n",
    "    scheduler.step()\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    print(f\"Epoch {epoch}/{epochs} — \"\n",
    "          f\"train_loss={train_loss:.5f}  val_loss={val_loss:.5f}  lr={lr:.1e}\")\n",
    "\n",
    "    # Checkpoint after each epoch.\n",
    "    # Notice we are only checkpointing the model encoder as it is is the sentence transformer.\n",
    "    enc_state = model.encoder.state_dict()\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'encoder_state_dict': enc_state,\n",
    "        'optimizer_state_dict': opt.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, os.path.join(checkpoint_dir, f\"encoder_epoch_{epoch}.pt\"))\n",
    "\n",
    "    torch.save(enc_state, os.path.join(checkpoint_dir, f\"epoch_{epoch}.pt\"))\n",
    "\n",
    "    # Early stopping to avoid overfitting\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        torch.save(model.encoder.state_dict(), \"best_encoder.pt\")\n",
    "        print(\"  ↳ New best Sentence Transformer saved.\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print(f\"  ↳ No improvement for {no_improve} epoch(s).\")\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Stopping early after {patience} epochs without improvement.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9e8d46f-b46b-4aca-97bf-066a3160d8e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos(+): 0.713  cos(-): -0.246\n",
      "cos(+): 0.730  cos(-): 0.385\n",
      "cos(+): 0.837  cos(-): 0.271\n",
      "cos(+): 0.684  cos(-): 0.211\n",
      "cos(+): 0.613  cos(-): -0.029\n",
      "cos(+): 0.886  cos(-): 0.088\n",
      "cos(+): 0.951  cos(-): 0.111\n",
      "cos(+): 0.935  cos(-): 0.150\n",
      "cos(+): 0.806  cos(-): -0.005\n",
      "cos(+): 0.947  cos(-): 0.580\n"
     ]
    }
   ],
   "source": [
    "# Sanity check on test data.\n",
    "# Notice the positive sentences have a greater similarity than the negative sentances when compared to the anchor sentences.\n",
    "for idx in np.random.randint(1,100,10):\n",
    "    (a, a_mask), (p, p_mask), (n, n_mask) = collate([test_dataset[idx]])\n",
    "\n",
    "    a_ids, a_mask = a.to(device), a_mask.to(device)\n",
    "    p_ids, p_mask = p.to(device), p_mask.to(device)\n",
    "    n_ids, n_mask = n.to(device), n_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        e_a, e_p, e_n = model(a, a_mask, p, p_mask, n, n_mask)\n",
    "        cos = F.cosine_similarity(e_a, e_p).item()\n",
    "        cos_neg = F.cosine_similarity(e_a, e_n).item()\n",
    "    \n",
    "    print(f\"cos(+): {cos:.3f}  cos(-): {cos_neg:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3462f-d7eb-43bb-bb2c-027e6d6d326f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
